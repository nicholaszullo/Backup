---
title: "CS 1675 Fall 2021 Homework: 10"
subtitle: "Assigned November 12, 2021; Due: November 19, 2021"
author: "Nicholas Zullo"
date: "Submission time: November 19, 2021 at 1:25PM EST"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Collaborators

No collaborators

## Overview

This homework assignment is focused on working with logistic regression, a type of generalized linear models. You will fit, make predictions with, and assess model performance. You will also derive the gradient vector of the log-posterior with respect to the unknown coefficients. This way you get first hand experience working with the derivatives, and thus the gradient of logistic regression.  

**IMPORTANT**: Problems 03 and 04 are associated with derivations. If you concerned about working through derivations of the gradient, start with the programming portions of the assignment for Problems 01, 02, and 05.  

**IMPORTANT**: code chunks are created for you. Each code chunk has `eval=FALSE` set in the chunk options. You **MUST** change it to be `eval=TRUE` in order for the code chunks to be evaluated when rendering the document.  

You are allowed to add as many code chunks as you see fit to answer the questions.  

## Load packages

This assignment will use packages from the `tidyverse` suite.  

```{r, load_packages}
library(tidyverse)
```


This assignment also uses the `splines` and `MASS` packages. Both are installed with base `R` and so you do not need to download any additional packages to complete the assignment.  

## Problem 01

You defined log-posterior functions for linear models in previous assignments. You worked with simple linear relationships, interactions, polynomials, and more complex spline basis features. In lecture, we discussed how the linear model framework can be *generalized* to handle non-continuous binary outcomes. The likelihood changed from a Gaussian to a Binomial distribution and a non-linear **link** function is required. In this way, the linear model is applied to a *linear predictor* which "behaves" like the trend in an ordinary linear model. In this problem, you will define the log-posterior function for logistic regression. By doing so you will be able to directly contrast what you did to define the log-posterior function for the linear model in previous assignments.  

### 1a)

The complete probability model for logistic regression consists of the likelihood of the response $y_n$ given the event probability $\mu_n$, the inverse link function between the probability of the event, $\mu_n$, and the linear predictor, $\eta_n$, and the prior on all linear predictor model coefficients $\boldsymbol{\beta}$.  

As in lecture, you will assume that the $\boldsymbol{\beta}$-parameters are a-priori independent Gaussians with a shared prior mean $\mu_{\beta}$ and a shared prior standard deviation $\tau_{\beta}$.  

**Write out complete probability model for logistic regression. You must write out the $n$-th observation's linear predictor using the inner product of the $n$-th row of a design matrix $\mathbf{x}_{n,:}$ and the unknown $\boldsymbol{\beta}$-parameter column vector. You can assume that the number of unknown coefficients is equal to $D + 1$.**  

You are allowed to separate each equation into its own equation block.  

*HINT*: The "given" sign, the vertical line, $\mid$, is created by typing `\mid` in a latex math expression. The product symbol (the giant PI) is created with `prod_{}^{}`.  

#### SOLUTION

Likelihood
$$
y | \mu_n \sim \prod_{n=1}^{N} Bernoulli(y_n | \mu_n)
$$
$$
\mu_n = logit^{-1}(\eta_n)
$$

Linear Predictor
$$
\eta_n = x_{n,:}\beta
$$
Prior
$$
p(\beta) = \prod_{d=0}^{D}(Normal(\beta_d | \mu_\beta, \tau_\beta))
$$

### 1b)

The code chunk below loads in a data set consisting of two variables. A continuous input `x` and a binary outcome `y`. The binary outcome is encoded as 0 if the event does not occur and 1 if the event does occur. The `count()` function is used to count the number of observations associated with each binary class in the second code chunk. As shown below, the event occurs more frequently than the non-event, but the two classes are not overly imbalanced.  

```{r, read_glm_dataset, eval=TRUE}
train_data_url <- 'https://raw.githubusercontent.com/jyurko/CS_1675_Fall_2021/main/HW/10/hw10_data_01.csv'
train_df <- readr::read_csv(train_data_url, col_names = TRUE)

train_df %>% glimpse()
```


```{r, check_outcome_count, eval=TRUE}
train_df %>% count(y)
```

You will fit three logistic regression models. The first will be a linear relationship for the linear predictor, the second will be a cubic polynomial relationship, and the third will be a 7 degree-of-freedom natural spline. The linear predictor for the linear relationship model is:  

$$ 
\eta_n = \beta_0 + \beta_1 x_n
$$

The linear predictor for the cubic polynomial is:  

$$ 
\eta_n = \beta_0 + \beta_1 x_n + \beta_2 x_{n}^2 + \beta_3 x_{n}^3
$$

Assuming the $j$-th spline feature is $\phi_j\left(x\right)$ the 7th degree of freedom natural spline can be written as a summation series:  

$$ 
\eta_n = \beta_0 + \sum_{j=1}^{J=7} \left( \phi_j \left(x_n\right) \right)
$$

You will be defining a log-posterior function in the style of those from previous assignments. However, before doing so, you will create lists of required information for each of the desired models. You must create the design matrices associated with the linear and cubic polynomials, and the 7 degree-of-freedom spline. You must assign those design matrices to the `Xmat_linear`, `Xmat_cubic`, and `Xmat_spline7` objects. You must then complete the lists `info_linear`, `info_cubic`, and `info_spline7` by setting the observed responses to the `yobs` variable, the design matrices to the `design_matrix` variable, and also set the $\boldsymbol{\beta}$ prior hyperparameters.  

**Create the design matrices for the 3 models and specify the lists of required information. Specify the prior mean to be 0 and the prior standard deviation to be 5.**  

#### SOLUTION

Create the design matrices.  

```{r, solution_01b_a, eval=TRUE}
Xmat_linear <- model.matrix(~x, data=train_df)

Xmat_cubic <- model.matrix(~x + I(x^2) + I(x^3), data=train_df)

Xmat_spline7 <- model.matrix( ~ splines::ns(x, df=7), data=train_df)
```


Create the lists of required information.  

```{r, solution_01b_b, eval=TRUE}
info_linear <- list(
  yobs = train_df$y,
  design_matrix = Xmat_linear,
  mu_beta = 0,
  tau_beta = 5
)

info_cubic <- list(
  yobs = train_df$y,
  design_matrix = Xmat_cubic,
  mu_beta = 0,
  tau_beta = 5
)

info_spline7 <- list(
  yobs = train_df$y,
  design_matrix = Xmat_spline7,
  mu_beta = 0,
  tau_beta = 5
)
```


### 1c)

You will now define the log-posterior function for logistic regression, `logistic_logpost()`. The first argument to `logistic_logpost()` is the vector of unknowns and the second argument is the list of required information. You will assume that the variables within the `my_info` list are those contained in the `info_linear` list you defined previously.  

**Complete the code chunk to define the `logistic_logpost()` function. The comments describe what you need to fill in. Do you need to separate out the $\boldsymbol{\beta}$-parameters from the vector of `unknowns`?**  

**After you complete `logistic_logpost()`, test it by setting the `unknowns` vector to be a vector of -1's and then 2's for the linear relationship case. If you have successfully programmed the function you should get `-164.6232` and `-130.1428` for the -1 test case and +2 test case, respectively.**  

#### SOLUTION

Do you need to separate the $\boldsymbol{\beta}$-parameters from the `unknowns` vector?  

We do not need to seperate the beta parameters because we are not including varphi at the end of the unknowns vector.

```{r, solution_01c, eval=TRUE}
logistic_logpost <- function(unknowns, my_info)
{
  # extract the design matrix and assign to X
  X <- my_info$design_matrix
  
  # calculate the linear predictor
  eta <- X %*% unknowns
  
  # calculate the event probability
  mu <- boot::inv.logit(eta)
  
  # evaluate the log-likelihood
  log_lik <- sum(dbinom(x = my_info$yobs,
                        size = 1,
                        prob = mu,
                        log = TRUE))
  
  # evaluate the log-prior
  log_prior <- sum(dnorm(x=unknowns,
                         mean = my_info$mu_beta,
                         sd = my_info$tau_beta,
                         log = TRUE))
  
  # sum together
  return(log_lik + log_prior)
}
```

Test out your function using the linear relationship information and setting the unknowns to a vector of -1's.  

```{r, solution_01c_b}
logistic_logpost(rep(-1, length.out=ncol(Xmat_linear)), info_linear)
```

Test out your function using the linear relationship information and setting the unknowns to a vector of 2's.  

```{r, solution_01c_c}
logistic_logpost(rep(2, length.out=ncol(Xmat_linear)), info_linear)
```

### 1d)

The `my_laplace()` function is provided to you in the code chunk below.  

```{r, define_my_laplace_func, eval=TRUE}
my_laplace <- function(start_guess, logpost_func, ...)
{
  # code adapted from the `LearnBayes`` function `laplace()`
  fit <- optim(start_guess,
               logpost_func,
               gr = NULL,
               ...,
               method = "BFGS",
               hessian = TRUE,
               control = list(fnscale = -1, maxit = 1001))
  
  mode <- fit$par
  post_var_matrix <- -solve(fit$hessian)
  p <- length(mode)
  int <- p/2 * log(2 * pi) + 0.5 * log(det(post_var_matrix)) + logpost_func(mode, ...)
  # package all of the results into a list
  list(mode = mode,
       var_matrix = post_var_matrix,
       log_evidence = int,
       converge = ifelse(fit$convergence == 0,
                         "YES", 
                         "NO"),
       iter_counts = as.numeric(fit$counts[1]))
}
```


You will use `my_laplace()` to execute the Laplace Approximation for the three models, using initial guess values of zero. After fitting the models, calculate the middle 95% uncertainty intervals for the linear and cubic model parameters.  

**Perform the Laplace Approximation for all three models. The linear relationship should be assigned to the `laplace_linear` object, the cubic relationship to the `laplace_cubic`, and the 7 degree-of-freedom spline should be assigned to the `laplace_spline7` object. Should you be concerned that the initial guess will impact the results?**  

**After fitting, calculate the middle 95% uncertainty interval on the $\boldsymbol{\beta}$ parameters for the linear and cubic models. Which parameters contain zero in the middle 95% uncertainty interval?**  

#### SOLUTION

What do you think?  

```{r, solution_01d_a, eval=TRUE}
laplace_linear <- my_laplace(rep(0, length.out=ncol(info_linear$design_matrix)), logistic_logpost, info_linear)

laplace_cubic <- my_laplace(rep(0, length.out=ncol(info_cubic$design_matrix)), logistic_logpost, info_cubic)

laplace_spline7 <- my_laplace(rep(0, length.out=ncol(info_spline7$design_matrix)), logistic_logpost, info_spline7)
```

Calculate the 95% uncertainty intervals on the linear relationship model.  

```{r, solution_01d_b}
laplace_linear$mode - 2 * sqrt(diag(laplace_linear$var_matrix))
laplace_linear$mode + 2 * sqrt(diag(laplace_linear$var_matrix))

```
The each column is a beta parameter, row 1 is the bottom of the uncertainty interval, and row 2 is the top of the uncertainty interval. 

Calculate the 95% uncertainty intervals on the cubic relationship model.  

```{r, solution_01d_c}
laplace_cubic$mode - 2 * sqrt(diag(laplace_cubic$var_matrix))
laplace_cubic$mode + 2 * sqrt(diag(laplace_cubic$var_matrix))
```
The only parameter that contains 0 in the uncertainty window is the intercept for the cubic model.

### 1e)

Let's compare the performance of the models using the Evidence-based assessment.  

**You must calculate the posterior model weight associated with each model. Based on your results, which model do you think is better?**  

#### SOLUTION

```{r, solution_01e}
sum_val <- exp(laplace_linear$log_evidence) + exp(laplace_cubic$log_evidence) + exp(laplace_spline7$log_evidence)
exp(laplace_linear$log_evidence)/sum_val
exp(laplace_cubic$log_evidence)/sum_val
exp(laplace_spline7$log_evidence)/sum_val
```
The cubic model is shown to be the best here. The model with weight closest to 1 is the picked model. 

## Problem 02

In Problem 01, you compared the linear and cubic relationships based on the Evidence. Your assessment considered how well the model "fit" the data via the likelihood, based on the constraints imposed by the prior. The likelihood examines how likely the binary outcome is given the event probability. Thus, the Evidence is considering if the observations are consistent with the modeled event probability. In this problem, you will consider point-wise error metrics by calculating the confusion matrix associated with the training set. Confusion matrices are useful because the accuracy and errors are in the same "units" of the data.  

However, the logistic regression model predicts the event probability via the log-odds ratio. In order to move from the probability to the binary outcome a decision must be made. As discussed during the Applied Machine Learning portion of the course, the decision consists of comparing the predicted probability to a threshold value. If the predicted probability is greater than the threshold, classify the outcome as the event. Otherwise, classify the outcome as the non-event.  

In order to classify the training points, you must make posterior predictions with the logistic regression models you fit in Problem 01.  

### 2a)

Although you were able to apply the `my_laplace()` function to both the regression and logistic regression settings, you cannot directly apply the `generate_lm_post_samples()` function from your previous assignments. You will therefore adapt `generate_lm_post_samples()` and define `generate_glm_post_samples()`. The code chunk below starts the function for you and uses just two input arguments, `mvn_result` and `num_samples`. You must complete the function.  

**Why can you not directly use the `generate_lm_post_samples()` function? Since the `length_beta` argument is NOT provided to `generate_glm_post_samples()`, how can you determine the number of $\boldsymbol{\beta}$-parameters? Complete the code chunk below by first assigning the number of $\boldsymbol{\beta}$-parameters to the `length_beta` variable. Then generate the random samples from the MVN distribution. You do not have to name the variables, you only need to call the correct random number generator.**  

#### SOLUTION

We cannot directly use the `generate_lm_post_samples()` because it includes the back-transformation from $\varphi$ to $\sigma$. The logistic regression model does **NOT** include $\sigma$. And so we should not use the function we used in the previous assignments. Since the only unknowns are the $\boldsymbol{\beta}$-parameters, we can determine `length_beta` by just using the length of the posterior mode vector.  

```{r, solution_02a, eval=TRUE}
generate_glm_post_samples <- function(mvn_result, num_samples)
{
  # specify the number of unknown beta parameters
  length_beta <- length(mvn_result$mode)
  
  # generate the random samples
  beta_samples <- MASS::mvrnorm(n = num_samples, 
                                mu = mvn_result$mode, 
                                Sigma = mvn_result$var_matrix)
  
  # change the data type and name
  beta_samples %>% 
    as.data.frame() %>% tibble::as_tibble() %>% 
    purrr::set_names(sprintf("beta_%02d", (1:length_beta) - 1))
}
```


### 2b)

You will now define a function which calculates the posterior prediction samples on the linear predictor and the event probability. The function, `post_logistic_pred_samples()` is started for you in the code chunk below. It consists of two input arguments `Xnew` and `Bmat`. `Xnew` is a test design matrix where rows correspond to prediction points. The matrix `Bmat` stores the posterior samples on the $\boldsymbol{\beta}$-parameters, where each row is a posterior sample and each column is a parameter.  

**Complete the code chunk below by using matrix math to calculate the linear predictor at every posterior sample. Then, calculate the event probability for every posterior sample.**  

The `eta_mat` and `mu_mat` matrices are returned within a list, similar to how the `Umat` and `Ymat` matrices were returned for the regression problems.  

*HINT*: The `boot::inv.logit()` can take a matrix as an input. When it does, it returns a matrix as a result.  

#### SOLUTION

```{r, solution_02b, eval=TRUE}
post_logistic_pred_samples <- function(Xnew, Bmat)
{
  # calculate the linear predictor at all prediction points and posterior samples
  eta_mat <- Xnew %*% t(Bmat)
  
  # calculate the event probability
  mu_mat <- boot::inv.logit(eta_mat)
  
  # book keeping
  list(eta_mat = eta_mat, mu_mat = mu_mat)
}
```


### 2c)

The code chunk below defines a function `summarize_logistic_pred_from_laplace()` which manages the actions necessary to summarize posterior predictions of the event probability. The first argument, `mvn_result`, is the Laplace Approximation object. The second object is the test design matrix, `Xtest`, and the third argument, `num_samples`, is the number of posterior samples to make. You must follow the comments within the function in order to generate posterior prediction samples of the linear predictor and the event probability, and then to summarize the posterior predictions of the event probability.  

The result from `summarize_logistic_pred_from_laplace()` summarizes the posterior predicted event probability with the posterior mean, as well as the 5th and 95th quantiles. If you have completed the `post_logistic_pred_samples()` function correctly, the dimensions of the `mu_mat` matrix should be consistent with those from the `Umat` matrix from the regression problems.  

The posterior summary statistics summarize over all posterior samples. You must therefore choose between `colMeans()` and `rowMeans()` as to how to calculate the posterior mean event probability for each prediction point. The posterior quantiles are calculated for you.  

**Follow the comments in the code chunk below to complete the definition of the `summarize_logistic_pred_from_laplace()` function. You must generate posterior samples, make posterior predictions, and then summarize the posterior predictions of the event probability.**  

*HINT*: The result from `post_logistic_pred_samples()` is a list.  

#### SOLUTION

```{r, solution_02c, eval=TRUE}
summarize_logistic_pred_from_laplace <- function(mvn_result, Xtest, num_samples)
{
  # generate posterior samples of the beta parameters
  betas <- generate_glm_post_samples(mvn_result, num_samples)
  
  # data type conversion
  betas <- as.matrix(betas)
  
  # make posterior predictions on the test set
  pred_test <- post_logistic_pred_samples(Xtest, betas)
  
  # calculate summary statistics on the posterior predicted probability
  # summarize over the posterior samples
  
  # posterior mean, should you summarize along rows (rowMeans) or 
  # summarize down columns (colMeans) ???
  mu_avg <- rowMeans(pred_test$mu_mat)

  # posterior quantiles
  mu_q05 <- apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.05)
  mu_q95 <- apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.95)

  # book keeping
  tibble::tibble(
    mu_avg = mu_avg,
    mu_q05 = mu_q05,
    mu_q95 = mu_q95
  ) %>% 
    tibble::rowid_to_column("pred_id")
}
```


### 2d)

Summarize the posterior predicted event probability associated with the training set all three models. After making the predictions, a code chunk is provided for you which generates a figure showing how the posterior predicted probability summaries compare with the observed binary outcomes. Which of the three models appear to better capture the trends in the binary outcome?  

**Call `summarize_logistic_pred_from_laplace()` for the all three models on the training set. Specify the number of posterior samples to be 2500. Print the dimensions of the resulting objects to the screen. How many rows are in each data set?**  

**The third code chunk below uses the prediction summaries to visualize the posterior predicted event probability on the training set. Which relationship seems more in line with the observations?**  

#### SOLUTION

THe prediction summarizes should be executed in the code chunk below.  

```{r, solution_02d_a, eval=TRUE}
set.seed(8123) 

post_pred_summary_linear <- summarize_logistic_pred_from_laplace(laplace_linear,info_linear$design_matrix ,2500)

post_pred_summary_cubic <- summarize_logistic_pred_from_laplace(laplace_cubic,info_cubic$design_matrix ,2500)

post_pred_summary_spline7 <-summarize_logistic_pred_from_laplace(laplace_spline7,info_spline7$design_matrix ,2500)
```

Print the dimensions of the objects to the screen.  

```{r, solution_02d_b}
dim(post_pred_summary_linear)
dim(post_pred_summary_cubic)
dim(post_pred_summary_spline7)
```

The figure below is created for you.  

```{r, solutioN_02d_c, eval=TRUE}
post_pred_summary_linear %>% 
  mutate(type = "linear relationship") %>% 
  bind_rows(post_pred_summary_cubic %>% 
              mutate(type = "cubic relationship")) %>% 
  bind_rows(post_pred_summary_spline7 %>% 
              mutate(type = "7 dof spline")) %>% 
  left_join(train_df %>% tibble::rowid_to_column("pred_id"),
            by = "pred_id") %>% 
  ggplot(mapping = aes(x = x)) +
  geom_ribbon(mapping = aes(ymin = mu_q05,
                            ymax = mu_q95,
                            group = type),
              fill = "steelblue", alpha = 0.5) +
  geom_line(mapping = aes(y = mu_avg,
                          group = type),
            color = "navyblue", size = 1.15) +
  geom_point(mapping = aes(y = y),
             size = 2.5, alpha = 0.2) +
  facet_grid( . ~ type) +
  labs(y = "y or event probability") +
  theme_bw()
```
The cubic function seems to properly account for the distribution of the data better than the other 2 models.

### 2e)

You will now consider classifying the predictions based upon a threshold value of 0.5. You will compare that threshold value to the posterior predicted event probabilities associated with the training set. Although the Bayesian model provides a full posterior predictive distribution, you will work just with the posterior mean value. Thus, you will create a single confusion matrix, rather than considering the uncertainty in the confusion matrix.  

Creating the confusion matrix is rather simple compared to some of the previous tasks in this assignment. The first step is to classify the prediction as event or non-event, which can be accomplished with an if-statement. The `ifelse()` function provides an "Excel-like" conditional statement, and is a simple way to perform the classification task. The syntax for `ifelse()` consists of three arguments, shown below:  

`ifelse(<conditional test>, <return if condition is TRUE>, <return if condition is FALSE>)`

The first argument is the conditional test you wish to apply. The second argument is what will be returned if the condition is true, and the third argument is what will be returned if the condition is false.  

You will use the `ifelse()` function to compare the posterior predicted mean event probability to the assumed threshold value of 0.5.  

**Pipe the `post_pred_summary_linear` object into a `mutate()` call and create a new variable `pred_class` which is the result of an `ifelse()` operation. For the conditional test, return a value of `1` if the posterior predicted mean event probability is greater than 0.5, and return `0` otherwise. Repeat the process for the `post_pred_summary_cubic` and `post_pred_summary_spline7` objects.**  

#### SOLUTION

```{r, solution_2e, eval=TRUE}
post_pred_summary_linear_class <- post_pred_summary_linear %>% mutate(pred_class=ifelse(mu_avg > .5, 1, 0))

post_pred_summary_cubic_class <- post_pred_summary_cubic %>% mutate(pred_class=ifelse(mu_avg > .5, 1, 0))

post_pred_summary_spline7_class <- post_pred_summary_spline7 %>% mutate(pred_class=ifelse(mu_avg > .5, 1, 0))
```

### 2f)

The code chunk below uses the `left_join()` function to merge the training data set, `train_df` with each of the posterior prediction summary objects. The results, `post_pred_summary_linear_class_b`, `post_pred_summary_cubic_class_b`, and `post_pred_summary_spline7_class_b` now have predicted classifications, `pred_class`, and observed outcomes `y`.  

```{r, merge_preds_and_train_obs, eval=TRUE}
post_pred_summary_linear_class_b <- post_pred_summary_linear_class %>% 
  left_join(train_df %>% tibble::rowid_to_column("pred_id"),
            by = "pred_id")

post_pred_summary_cubic_class_b <- post_pred_summary_cubic_class %>% 
  left_join(train_df %>% tibble::rowid_to_column("pred_id"),
            by = "pred_id")

post_pred_summary_spline7_class_b <- post_pred_summary_spline7_class %>% 
  left_join(train_df %>% tibble::rowid_to_column("pred_id"),
            by = "pred_id")
```


**Use the `count()` function to determine the confusion matrix associated with each relationship. How many true-positives, true-negatives, false-positives, and false-negatives does each relationship have?**  

#### SOLUTION

The confusion matrix for the linear relationship is shown below.  

```{r, solution_02f_a}
post_pred_summary_linear_class_b %>% count(pred_class, y)
```
This has 16 True negatives, 63 true positives, 12 false negatives and 34 false positives.

The confusion matrix for the cubic relationship is shown below.  

```{r, solution_02f_b}
post_pred_summary_cubic_class_b %>% count(pred_class, y)
```
This has 31 True negatives, 57 true positives, 18 false negatives and 19 false positives.

The confusion matrix for the 7 degree-of-freedom spline is shown below.  

```{r, solution_02f_c}
post_pred_summary_spline7_class_b %>% count(pred_class, y)
```
This has 29 True negatives, 62 true positives, 13 false negatives and 21 false positives.

## Problem 03

In lecture, we worked through the derivation of the gradient of the log-posterior associated with logistic regression assuming an infinitely diffuse prior on the $\boldsymbol{\beta}$ parameters. Thus, the gradient is equivalent to the gradient of the log-likelihood. You will now consider the case of an informative or regularizing prior. The un-normalized log-posterior on the unknown coefficients given the observations is therefore:  

$$ 
\log \left( p \left( \boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X} \right) \right) \propto \log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right) + \log \left( p \left( \boldsymbol{\beta} \right) \right)
$$

In lecture, we worked through the derivations for the general case with $D$ inputs or predictors. For this problem however, you will consider a linear relationship associated with a single input, $x$. The $n$-th observation's linear predictor, $\eta_n$, is thus:  

$$ 
\eta_n = \beta_0 + \beta_1 x_n
$$

You will assume indepedent Gaussian priors on the intercept, $\beta_0$, and the slope, $\beta_1$, with prior mean $\mu_{\beta} = 0$ and prior standard deviation $\tau_{\beta} = b$. Thus, your final expressions should involve the $b$ hyperparameter.  

Your goal is to derive the gradient of the log-posterior with respect to the intercept, $\beta_0$, and the slope $\beta_1$. You will work through that derivation in steps.  

### 3a)

You will start by considering the derivatives of the log-prior, $\log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)$, with respect to the coefficients.  

**Derive the partial derivative of the log-prior with respect to the intercept $\beta_0$.**  

*HINT*: You can write the partial derivative as:  

$$ 
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)  \right)
$$

#### SOLUTION

$$
\log(p(\beta_0 | 0, b)) = \log\left( \frac{1}{b\sqrt{2\pi}} exp\left(\frac{-1}{2}\left(\frac{\beta_0}{b}\right)^2\right) \right) \\
\log(p(\beta_0 | 0, b)) = \log\left( \frac{1}{b\sqrt{2\pi}} \right) + \frac{-1}{2}\left(\frac{\beta_0}{b}\right)^2 \\ 
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta_0} \mid 0, b \right) \right)  \right) = \frac{\partial}{\partial \beta_0} \left(\frac{-1}{2}\left(\frac{\beta_0}{b}\right)^2 \right) \\
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta_0} \mid 0, b \right) \right)  \right) = -\frac{\beta_0}{b^2}
$$

### 3b)

**Derive the partial derivative of the log-prior with respect to the slope $\beta_1$.**  

#### SOLUTION

$$
\log(p(\beta_1 | 0, b)) = \log\left( \frac{1}{b\sqrt{2\pi}} exp\left(\frac{-1}{2}\left(\frac{\beta_1x_n}{b}\right)^2\right) \right) \\
\log(p(\beta_1 | 0, b)) = \log\left( \frac{1}{b\sqrt{2\pi}} \right) + \frac{-1}{2}\left(\frac{\beta_1x_n}{b}\right)^2 \\ 
\frac{\partial}{\partial \beta_1} \left( \log \left( p \left( \boldsymbol{\beta_1} \mid 0, b \right) \right)  \right) = \frac{\partial}{\partial \beta_1} \left(\frac{-1}{2}\left(\frac{\beta_1x_n}{b}\right)^2 \right) \\
\frac{\partial}{\partial \beta_1} \left( \log \left( p \left( \boldsymbol{\beta_1} \mid 0, b \right) \right)  \right) = -\frac{\beta_1x_n}{b^2}
$$
### 3c)

You will now consider the log-likelihood. You will start by considering the $n$-th observation's contribution to the log-likelihood, $L_n$.  

**Write out the expression for the $n$-th observation's log-likelihood in terms of the binary outcome $y_n$ and the event probability $\mu_n$.**  

#### SOLUTION

$$
L_n = y_n\log(\mu_n) + (1- y_n)\log(1-\mu_n)
$$

### 3d)

**Write out the partial derivative of $L_n$ with respect to the intercept using the chain rule, considering the event probability, $\mu_n$, and the linear predictor, $\eta_n$.**  

#### SOLUTION

$$ 
\frac{\partial L_n}{\partial \beta_0} = \frac{\partial L_n}{\partial \mu_n}\frac{\partial \mu_n}{\partial \eta_n}\frac{\partial \eta_n}{\partial \beta_0}
$$

### 3e)

**Use the terms of the chain rule to derive the partial first derivative of $L_n$ with respect to the intercept $\beta_0$. You should show the derivatives associated with each term in the chain rule.**  

#### SOLUTION
First part
$$
\frac{\partial L_n}{\partial \mu_n} = \frac{\partial}{\partial \mu_n}\left( y_n\log(\mu_n) + (1- y_n)\log(1-\mu_n) \right)\\
\frac{\partial L_n}{\partial \mu_n} =\frac{y_n}{\mu_n} - \frac{1-y_n}{1-\mu_n}
$$
Second part
$$
\frac{\partial \mu_n}{\partial \eta_n} = \frac{\partial }{\partial \eta_n}\left(\frac{exp(\eta_n)}{1+exp(\eta_n)}\right)\\
\frac{\partial \mu_n}{\partial \eta_n} = \mu_n(1-\mu_n)
$$
Third Part, intercept
$$
\frac{\partial \eta_n}{\partial \beta_0} = \frac{\partial}{\partial \beta_0}(\beta_0 + \beta_1x_n) \\
\frac{\partial \eta_n}{\partial \beta_0} = 1
$$


### 3f)

**The partial derivative you determined in 3e) is associated with just a single observation. Write out the expression for the partial derivative of "complete log-likelihood" with respect to the intercept.**  

#### SOLUTION

$$ 
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right)  \right) = \left(\frac{y_n}{\mu_n} - \frac{1-y_n}{1-\mu_n}\right)\mu_n(1-\mu_n)*1
$$

### 3g)

**Combine your result in 3f) with the partial derivative of the log-prior with respect to the intercept from 3a) to write out the partial first derivative of the log-posterior with respect to the intercept.**  

#### SOLUTION

Include as many equation blocks as you feel are necessary. The equation block below shows how to write out the partial derivative as a hint.  

$$ 
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X} \right) \right) \right) =  \left(\frac{y_n}{\mu_n} - \frac{1-y_n}{1-\mu_n}\right)\mu_n(1-\mu_n)*1 *\left(-\frac{\beta_0}{b^2}\right)
$$

## Problem 04

Problem 3) mostly focused on the derivatives with respect to the intercept, $\beta_0$. You will complete the derivation by considering the derivatives with respect to the slope, $\beta_1$.  

### 4a)

**Derive the partial first derivative of the log-likelihood with respect to the slope, $\beta_1$. If you feel can make use of any "patterns" from you derivations in Problem 3), state what those are.**  

#### SOLUTION



The first 2 equation blocks are identical to the intercept because they do not involve $\beta_0$, the difference is in the 3rd block.

$$ 
\frac{\partial L_n}{\partial \beta_0} = \frac{\partial L_n}{\partial \mu_n}\frac{\partial \mu_n}{\partial \eta_n}\frac{\partial \eta_n}{\partial \beta_0}
$$

First part
$$
\frac{\partial L_n}{\partial \mu_n} = \frac{\partial}{\partial \mu_n}\left( y_n\log(\mu_n) + (1- y_n)\log(1-\mu_n) \right)\\
\frac{\partial L_n}{\partial \mu_n} =\frac{y_n}{\mu_n} - \frac{1-y_n}{1-\mu_n}
$$
Second part
$$
\frac{\partial \mu_n}{\partial \eta_n} = \frac{\partial }{\partial \eta_n}\left(\frac{exp(\eta_n)}{1+exp(\eta_n)}\right)\\
\frac{\partial \mu_n}{\partial \eta_n} = \mu_n(1-\mu_n)
$$

Third part, slope
$$
\frac{\partial \eta_n}{\partial \beta_1} = \frac{\partial}{\partial \beta_1}(\beta_0 + \beta_1x_n) \\
\frac{\partial \eta_n}{\partial \beta_1} = x_n
$$

$$ 
\frac{\partial}{\partial \beta_1} \left( \log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right) \right) = \left(\frac{y_n}{\mu_n} - \frac{1-y_n}{1-\mu_n}\right)\mu_n(1-\mu_n)*x_n
$$

### 4b)

**Combine your result in 4a) with your result from 3b) to write out the partial first derivative of the log-posterior with respect to the slope.**  

#### SOLUTION

Include as many equation blocks as you feel are necessary. The equation block below shows how to write out the partial derivative as a hint.  

$$ 
\frac{\partial}{\partial \beta_1} \left( \log \left( p \left( \boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X} \right) \right) \right) = \left(\frac{y_n}{\mu_n} - \frac{1-y_n}{1-\mu_n}\right)\mu_n(1-\mu_n)*x_n *\left(-\frac{\beta_1x_n}{b^2}\right)
$$

### 4c)

**What impact does the prior have on the gradient relative to the gradient associated with the log-likelihood alone?**  

#### SOLUTION

There is now an additional term of $\frac{-1}{b^2}$ inside the gradient. This causes it to shift, but will not affect the shape of the curve because $\beta$ is not present. 

## Problem 05

In lecture, we spent a considerable amount of time discussing a situation which challenges fitting logistic regression with maximum likelihood estimation. Now that you have fit several logistic regression models, and examined the influence of the prior on the gradient, let's get some practice working with this challenging situation.  

A small data set is read for you in the code chunk below, consisting of a single input, `x`, and a binary outcome `y`. As shown by the `glimpse()` this data set consists of just 5 observations.  

```{r, read_small_data_05}
small_data_url <- "https://raw.githubusercontent.com/jyurko/CS_1675_Fall_2021/main/HW/10/hw10_data_small.csv"

train_small <- readr::read_csv( small_data_url )

train_small %>% glimpse()
```

### 5a)

**Plot the binary outcome, `y`, with respect to the continuous input, `x`. Include an additional layer to your graphic by adding in geom_vline() with the `xintercept` argument set to -0.6.**  

**Based on the figure, should we expect the logistic regression model to struggle with this small data set?**  

#### SOLUTION

```{r, solution_05a}
train_small %>% ggplot(mapping=aes(y=y, x=x)) + geom_point() + geom_vline(xintercept=-.6) 
```
The data is separated nicely, the model should be able to fit this properly. However, there are very few data points. Logistic regression models struggle with low amounts of data.

### 5b)

You will fit a Bayesian logistic regression model assuming a linear relationship between the input and the linear predictor:  

$$ 
\eta_n = \beta_0 + \beta_1 x_{n}
$$

You will fit two models using the linear relationship. The first will use an informative prior on the unknown $\boldsymbol{\beta}$ coefficients with a prior standard deviation of 2.5. The second model will use an very diffuse or *very weak* prior with a prior standard deviation of 50.  

Before fitting the models, you must create the lists of required information for this application. 

**Complete the code chunk below which creates the lists of required information for the informative and very weak priors using the small data set of just 5 observations.**  

**The informative prior should use the prior standard deviation of 2.5 and the very weak prior should use a prior standard deviation of 50. Both priors should use prior mean of 0.**  

#### SOLUTION

```{r, solution_05b, eval=TRUE}
info_small_inform <- list(
  yobs = train_small$y,
  design_matrix = model.matrix(~x, data=train_small),
  mu_beta = 0,
  tau_beta = 2.5
)

info_small_weak <- list(
  yobs = train_small$y,
  design_matrix = model.matrix(~x, data=train_small),
  mu_beta = 0,
  tau_beta = 50
)
```


### 5c)

You will now execute the Laplace Approximation to fit the Bayesian logistic regression models associated with the informative and weak priors.  

**Execute the laplace approximation for the two prior specifications. Assign the result associated with the informative prior to `small_laplace_from_inform` and the assign the result associated with the very weak prior to `small_laplace_from_weak`.**  

**How do the posterior modes compare between the two models? Does the informative prior cause the posterior mode to be different from the mode associated with the very weak prior?**  

#### SOLUTION

```{r, solution_05c_a, eval=TRUE}
small_laplace_from_inform <- my_laplace(rep(0, length.out = ncol(info_small_inform$design_matrix)), logistic_logpost, info_small_inform)
```


```{r, solution_05c_b, eval=TRUE}
small_laplace_from_weak <- my_laplace(rep(0, length.out = ncol(info_small_weak$design_matrix)), logistic_logpost, info_small_weak)
```

How do the posterior modes compare? Include text and code chunks to answer this question.  

```{r, sol_5c}
small_laplace_from_inform$mode
small_laplace_from_weak$mode
```
The modes are very far apart from each other. There is not enough data to pull these values closer together. The informative prior pulls the mode down and does not allow the large values like the weak prior

### 5d)

**Compare the posterior correlation between the intercept and slope associated with an informative prior to the posterior correlation associated with the very weak prior.**  

*HINT*: The Laplace Approximation gives you something that lets you easily calculate the correlation matrix...  

#### SOLUTION

```{r, sol_5d_a}
cov2cor(small_laplace_from_inform$var_matrix)
```
```{r, sol_5d_b}
cov2cor(small_laplace_from_weak$var_matrix)
```

### 5e)

**Discuss the differences between the posterior correlation between the coefficients when an informative prior is used compared to a very weak prior on this very small data set.**  

#### SOLUTION

The cross correlation is large with the weak prior, .95 is very close to 1. However, with the informative prior, these correlations are not able to form with only a few data points.

### 5f)

What do you think will happen if you would find the MLEs for the coefficients, and thus have no prior influence at all?  

**Use `glm()` to fit the logistic regression model via maximum likelihood estimation for the small data set. Assign the result to `mod_small_mle`.**  

**Display the coefficient MLEs, what's going on with the maximum likelihood result for the small data set?**  

#### SOLUTION

```{r, solution_05f}
mod_small_mle <- glm(y ~ x, family="binomial", data=train_small)
```

```{r, sol_5f_b}
mod_small_mle
```

The coefficient values are very large and do not seem to match the values of the data from the plot. The model did not have enough data to converge and find the proper values of the MLEs. 